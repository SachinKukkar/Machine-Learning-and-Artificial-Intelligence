{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Vs ML Vs DL Vs DS\n",
    "\n",
    "** Final goal is to create an AI application , whatever you may do \n",
    "\n",
    "AI - AI application is able to do its own task without any human intervention\n",
    "\n",
    "example - Netflix -> Action Movide dekhi aapne -> recommends more action movies\n",
    "          Amazon -> You bought iPhone -> recommends earphones etc.\n",
    "          Self Driving cars -> Tesla\n",
    "\n",
    "\n",
    "ML - Machine Learning is a subset of Artificial Intelligence,  it provides stats tools to analyse , visualize , prediction and forecasting of data\n",
    "\n",
    "DL - Deep Learning is a subset of Machine Learning, the plan is to mimic the human brain - we use multi-layered neural network - help you to train machine learning applications \n",
    "\n",
    "Data Science - It is part of ML , DL and AI\n",
    "\n",
    "Machine Learning and Deep Learning - 1.Supervised Learning and 2.Unsupervised Learning\n",
    "\n",
    "1.Supervised Learning -> Regression and Classification \n",
    "\n",
    "2.Unsupervised Learning -> Clustering and Dimensionality Reduction\n",
    "\n",
    "Supervised Learning -> Independent features - the input we gave\n",
    "                      -> Dependent features - the output we predict (dependent on input)\n",
    "\n",
    "1.1 Regression -> Output hamesha ek Continous variable hoga , output (y-axis) & input(x-axis) ka graph straight line banega \n",
    "\n",
    "1.2 Classification -> Output has fixed number of categories , if two categories , then it becomes binary classification \n",
    "\n",
    "Unsupervised Learning -> No Dependent variable in Clustering\n",
    "\n",
    "2.1 Clustering (Grouping) -> If we do customer segmentation , on the basis of similar data , we try to find similar groups and these groups are called as Clusters.Each and every cluster specifies an information.\n",
    "\n",
    "2.2 Dimensionality Reduction -> If we have 1000 features , we can reduce these features to lower dimensions using algorithms like PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Linear Regression : \n",
    "\n",
    "We try to create a model with the help of training dataset which has an independent variable and one dependent variable\n",
    "\n",
    "Straight Line graph bnega -> y is a linear function of x , we have to create a best-fit linear line\n",
    "\n",
    "Equation of a st. lin e: \n",
    "y = mx+c\n",
    "y = betazero + beta-one*x \n",
    "\n",
    "h-theta(x) = theta-zero + theta-one*x (this is hypothesis)\n",
    "\n",
    "h-theta(x) = theta-zero + theta-one*x\n",
    "\n",
    "theta-zero = Intercept (at x = 0) -> the point where we are meeting the y-axis\n",
    "\n",
    "theta-one = Slope or Coefficient (What is the unit movement in y-axis when we move a unit on x-axis)\n",
    "\n",
    "x = data points\n",
    "\n",
    "\n",
    "** Summation of differences between the actual point and the predicted point should be minimum for finding a best-fitted line\n",
    "\n",
    "To find the best fitted line, we have to keep changing the values of theta-zero and theta-one\n",
    "\n",
    "Cost function , J(theta-zero,theta-one) = 1/2m * (Summation of {(h-theta(x)-y) ^2 })\n",
    "\n",
    "The equation written above is called as \"SQUARED ERROR FUNCTION\"\n",
    "\n",
    "Our goal is to minimize -> 1/2m * (Summation of {(h-theta(x)-y) ^2 }) : \n",
    "\n",
    "\n",
    "h-theta(x) = theta-zero + theta-one*x (this is hypothesis)\n",
    "\n",
    "* if theta-zero = 0, best-fit line passes through origin and h-theta(x) = theta-one*x\n",
    "\n",
    "New hypothesis -> h-theta(x) = theta-one*x\n",
    "\n",
    "if theta-one = 1 , then J(theta-one) = 0 \n",
    "\n",
    "if theta-one = 0.5 , then J(theta-one) = 0.58 \n",
    "\n",
    "if theta-one = 0 , then J(theta-one) = 2.3\n",
    "\n",
    "similarly , when draw the graph of cost function here , we will get a parabolic shape and this graph is called as the \"GRADIENT DESCENT\"\n",
    "\n",
    "Our goal is to reach the 'Global minima' in the graph of Gradient Descent.\n",
    "\n",
    "\n",
    "# Convergence Algorithm : \n",
    "\n",
    "Repeat until convergence \n",
    "\n",
    "theta-j = (theta-j) - alpha {d/d-theta-j (J(theta-zero,theta-one))}\n",
    "\n",
    "Alpha is called as \"LEARNING RATE\" -> by what is we are coming from a point towards the global minima , so we should keep Alpha as minimum as possible.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT DESCENT ALGORITHM\n",
    "\n",
    "Repeat until convergence\n",
    "\n",
    "theta-j = (theta-j) - alpha {d/d-theta-j (J(theta-zero,theta-one))}\n",
    "\n",
    "d/d-theta-j (J(theta-zero,theta-one)) = d/d-theta-j {1/2m (summation of {h-theta(x)-y}^2)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance metrics\n",
    "\n",
    "R^2 and Adjusted R^2\n",
    "\n",
    "R^2 = 1 - (SS-residual/SS-total) = 1 - summation of (yi-yi-cap)^2/summation of (yi-y-bar)^2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
